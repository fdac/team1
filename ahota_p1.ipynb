{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      ":"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 166
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# I had to edit extract_data()\n",
      "# Previously, I had two dictionaries: one for users and one for repos\n",
      "# But I realized repos are NOT guaranteed to be uniquely named, which is\n",
      "# why we were getting fewer than we should've when searching.\n",
      "# The 'full_name' in json_response IS guaranteed to be unique since it includes\n",
      "# user's name, which is also guaranteed to be unique.\n",
      "# So now there is only one main dict, users, which contains all info.\n",
      "# users[username] is a nested dictionary and is just the names of that user's repos\n",
      "# as the key, and the rest of the json_response dict as the value.\n",
      "# Yes, triple-nested dictionaries.\n",
      "\n",
      "def extract_data(repo):\n",
      "    global users\n",
      "    username, slash, reponame = repo['full_name'].partition('/')\n",
      "    try:\n",
      "        users[username][reponame] = repo\n",
      "    except KeyError:\n",
      "        users[username] = {}\n",
      "        users[username][reponame] = repo\n",
      "\n",
      "\n",
      "def get_next_page():\n",
      "    global json_response\n",
      "    items = json_response['values']\n",
      "    for item in items:\n",
      "        extract_data(item)\n",
      "    json_response = requests.get(json_response['next']).json()\n",
      "\n",
      "\n",
      "def query(num):\n",
      "    global json_response\n",
      "    global cur_page\n",
      "    if num == 0:\n",
      "        # Go until there is nothing left!\n",
      "        while json_response.has_key('next'):\n",
      "            if cur_page % 100 == 0:\n",
      "                print 'On page', cur_page\n",
      "            get_next_page()\n",
      "            cur_page += 1\n",
      "            for entry in range(len(json_response['values'])):\n",
      "                if json_response['values'][entry]['created_on'][:13] == '2014-09-23T17':\n",
      "                    print 'Found repos created up to 5pm 9/23/2014'\n",
      "                    print 'Pages crawled:', cur_page\n",
      "                    return\n",
      "    else:\n",
      "        for i in range(num):\n",
      "            get_next_page()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 169
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# This is what the following lines do:\n",
      "#  - Try to load a pickled file from the disk\n",
      "#    - This is just a cache of the users previously found to save time. Delete these files to search again\n",
      "#  - If the file is there, it just loads it\n",
      "#  - If the file is NOT there, then it uses BitBucket's API to query DEF_QUERY number of pages\n",
      "#    - After it does this, it will save a pickled version of the dictionary it generates to be used as cache above\n",
      "\n",
      "\n",
      "try:\n",
      "    users = pickle.load( open('./users_cache/users.p', 'r') )\n",
      "    print '-- Using cached users --'\n",
      "except IOError:\n",
      "    print '-- Grabbing', DEF_QUERY, 'pages --'\n",
      "    query(0)\n",
      "    pickle.dump( users, open('./ext_cache/users.p', 'w') )\n",
      "    print '-- Results cached --'\n",
      "\n",
      "print 'Number of users found:', len(users.keys())\n",
      "print 'Number of repos found:', sum([len(users[user].keys()) for user in users.keys()])\n",
      "# Number of repos should be (number of queries) * (per_page)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "-- Grabbing 1000 pages --\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 100\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 200\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 300\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 400\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 500\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 600\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 700\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 800\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 900\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1000\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1100\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1200\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1300\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1400\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1500\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1600\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1700\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1800\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 1900\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2000\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2100\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2200\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2300\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2400\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2500\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2600\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2700\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2800\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 2900\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3000\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3100\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3200\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3300\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3400\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3500\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3600\n",
        "On page"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " 3700\n",
        "On page"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import collections, csv, operator\n",
      "\n",
      "# This loop gives us the languages from repos that have a specified language\n",
      "# We can make a graph based on this, but mention that a lot of repos don't specify language\n",
      "\n",
      "# Can you tell I like list comprehensions?\n",
      "languages = [users[user][repo]['language'] for user in users.keys() for repo in users[user].keys() if users[user][repo]['language']]\n",
      "lang_count = collections.Counter(languages)\n",
      "writer = csv.writer(open('languages.csv', 'w'))\n",
      "for key, value in lang_count.items():\n",
      "    writer.writerow([key, value])\n",
      "\n",
      "print 'Language use'\n",
      "for lang in lang_count.keys():\n",
      "    print '{0:16} {1:6}'.format(lang, lang_count[lang])\n",
      "print '----------------'\n",
      "\n",
      "# This loop gets the number of repos per user\n",
      "# What we're showing in this counter is the number of users who had <n> repos\n",
      "\n",
      "num_repos_per_user = [ len(users[user].keys()) for user in users.keys() ]\n",
      "num_repos_count = collections.Counter(num_repos_per_user)\n",
      "writer = csv.writer(open('num_repos.csv', 'w'))\n",
      "for key, value in num_repos_count.items():\n",
      "    writer.writerow([key, value])\n",
      "\n",
      "print 'Number of repos'\n",
      "for repo in num_repos_count.keys():\n",
      "    print '{0:3} {1:4}'.format(repo, num_repos_count[repo])\n",
      "print '----------------'\n",
      "\n",
      "# This gets the years repos were created\n",
      "# Then we get a count of how often each year was listed\n",
      "\n",
      "year_created = [ users[user][repo]['created_on'][:4] for user in users.keys() for repo in users[user].keys() ]\n",
      "year_count = collections.Counter(year_created)\n",
      "f = open('year_created.csv', 'w')\n",
      "writer = csv.writer(f)\n",
      "for year in year_count.keys():\n",
      "    writer.writerow([year, year_count[year]])\n",
      "f.close()\n",
      "\n",
      "print 'Year created'\n",
      "for year in year_count.keys():\n",
      "    print '{0:4} {1:4}'.format(year, year_count[year])\n",
      "print '-----------------'\n",
      "\n",
      "# Size of repo binned to 100MB\n",
      "\n",
      "sizes = [ users[user][repo]['size'] for user in users.keys() for repo in users[user].keys() ]\n",
      "sizes.sort()\n",
      "binned_sizes = [size/100000000 for size in sizes]\n",
      "sizes_dict = {}\n",
      "for b in binned_sizes:\n",
      "    if b < 10:\n",
      "        try:\n",
      "            sizes_dict[b] += 1\n",
      "        except KeyError:\n",
      "            sizes_dict[b] = 1\n",
      "    else:\n",
      "        try:\n",
      "            sizes_dict[10] += 1\n",
      "        except KeyError:\n",
      "            sizes_dict[10] = 1\n",
      "\n",
      "f = open('overall_sizes.csv', 'w')\n",
      "writer = csv.writer(f)\n",
      "for s in sizes_dict.keys():\n",
      "    writer.writerow([s, sizes_dict[s]])\n",
      "f.close()\n",
      "\n",
      "print 'Overall size (100MB)'\n",
      "for size in sizes_dict.keys():\n",
      "    print '{0:3} {1:5}'.format(size, sizes_dict[size])\n",
      "print '--------------'\n",
      "\n",
      "\n",
      "#sorted_lang = sorted(lang_count.items(), key=operator.itemgetter(1))[::-1][:10]\n",
      "#top_langs = [lang[0] for lang in sorted_lang]\n",
      "#sizes_by_lang = {}\n",
      "#for lang in top_langs:\n",
      "#    sizes_by_lang[lang] = [ users[user][repo]['size'] for user in users.keys() for repo in users[user].keys() if users[user][repo]['language'] == lang ]  \n",
      "#\n",
      "#print sorted(sizes_by_lang['python'])[::-1][:10]\n",
      "\n",
      "lang_by_year = {}\n",
      "for year in range(2008, 2013):\n",
      "    lang_by_year[year] = [ users[user][repo]['language'] for user in users.keys() for repo in users[user].keys() if int(users[user][repo]['created_on'][:4]) == year ]\n",
      "\n",
      "counted_lang_by_year = {}\n",
      "for year in range(2008, 2013):\n",
      "    counted_lang_by_year[year] = collections.Counter(lang_by_year[year])\n",
      "\n",
      "print sorted(counted_lang_by_year[2008].items(), key=operator.itemgetter(1))[::-1][1:10]\n",
      "print sorted(counted_lang_by_year[2009].items(), key=operator.itemgetter(1))[::-1][1:10]\n",
      "print sorted(counted_lang_by_year[2010].items(), key=operator.itemgetter(1))[::-1][1:10]\n",
      "print sorted(counted_lang_by_year[2011].items(), key=operator.itemgetter(1))[::-1][1:10]\n",
      "print sorted(counted_lang_by_year[2012].items(), key=operator.itemgetter(1))[::-1][1:10]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Language use\n",
        "qml                   1\n",
        "tcl                  30\n",
        "go                  377\n",
        "lasso                 1\n",
        "xml                 189\n",
        "arduino             107\n",
        "restructuredtext      2\n",
        "pure data            43\n",
        "fortran              56\n",
        "matlab              167\n",
        "ada                  50\n",
        "erlang              124\n",
        "limbo                 5\n",
        "python            11968\n",
        "viml                205\n",
        "dart                  1\n",
        "d                    93\n",
        "abap                  1\n",
        "common lisp         145\n",
        "xquery                4\n",
        "unrealscript          5\n",
        "html/css           1444\n",
        "unityscript           1\n",
        "vhdl                 24\n",
        "tex                  46\n",
        "arc                   6\n",
        "c++                4707\n",
        "racket               26\n",
        "scala               386\n",
        "delphi              152\n",
        "lua                 308\n",
        "verilog              23\n",
        "forth                 8\n",
        "javascript         2712\n",
        "assembly             96\n",
        "vala                 17\n",
        "processing           49\n",
        "octave                1\n",
        "swift                 1\n",
        "c                  2678\n",
        "visualbasic           1\n",
        "coldfusion           24\n",
        "vb.net              134\n",
        "ocaml               154\n",
        "boo                   3\n",
        "clojure             163\n",
        "markdown             76\n",
        "standardml            7\n",
        "haskell             397\n",
        "puppet               17\n",
        "powerbasic            1\n",
        "perl                649\n",
        "sourcepawn           44\n",
        "java               7246\n",
        "foxpro               10\n",
        "object pascal        55\n",
        "euphoria              9\n",
        "haxe                 36\n",
        "scheme              109\n",
        "oxygene               4\n",
        "shell               951\n",
        "ooc                   1\n",
        "c#                 4235\n",
        "ruby               1286\n",
        "groovy              212\n",
        "xpages                4\n",
        "labview              25\n",
        "r                   122\n",
        "coco                  3\n",
        "zsh                  16\n",
        "powershell           36\n",
        "prolog               18\n",
        "emacs lisp          189\n",
        "objective-c         715\n",
        "f#                  120\n",
        "objective-j           5\n",
        "smalltalk             8\n",
        "self                  7\n",
        "mathematica           9\n",
        "max/msp               2\n",
        "pl/sql                3\n",
        "supercollider         5\n",
        "coffeescript        116\n",
        "other                11\n",
        "actionscript        371\n",
        "android              23\n",
        "nu                    1\n",
        "css                  31\n",
        "duby                  2\n",
        "componentpascal       1\n",
        "eiffel                5\n",
        "sql                 105\n",
        "php                4005\n",
        "fantom               38\n",
        "latex               451\n",
        "sass                 14\n",
        "m                     2\n",
        "inform                3\n",
        "----------------\n",
        "Number of repos"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  1 26480\n",
        "  2 6743\n",
        "  3 3122\n",
        "  4 1705\n",
        "  5  991\n",
        "  6  710\n",
        "  7  519\n",
        "  8  368\n",
        "  9  265\n",
        " 10  193\n",
        " 11  166\n",
        " 12  142\n",
        " 13  117\n",
        " 14  105\n",
        " 15   79\n",
        " 16   61\n",
        " 17   41\n",
        " 18   53\n",
        " 19   39\n",
        " 20   27\n",
        " 21   38\n",
        " 22   25\n",
        " 23   20\n",
        " 24   13\n",
        " 25   15\n",
        " 26   12\n",
        " 27    9\n",
        " 28   14\n",
        " 29   16\n",
        " 30   11\n",
        " 31   11\n",
        " 32   10\n",
        " 33   10\n",
        " 34    9\n",
        " 35    8\n",
        " 36   13\n",
        " 37    2\n",
        " 38    6\n",
        " 39    4\n",
        " 40    3\n",
        " 41    7\n",
        " 42    3\n",
        "135    1\n",
        " 44    3\n",
        " 45    3\n",
        " 46    1\n",
        " 47    1\n",
        " 48    2\n",
        " 49    3\n",
        " 50    3\n",
        " 51    2\n",
        " 52    2\n",
        " 54    2\n",
        " 55    1\n",
        "132    1\n",
        " 58    2\n",
        " 59    1\n",
        " 60    1\n",
        " 61    1\n",
        " 62    1\n",
        "191    1\n",
        "139    1\n",
        " 69    4\n",
        " 71    1\n",
        " 72    2\n",
        " 76    2\n",
        " 78    2\n",
        " 79    1\n",
        " 81    1\n",
        " 83    2\n",
        " 43    2\n",
        "219    1\n",
        " 93    1\n",
        " 94    1\n",
        "165    1\n",
        " 67    1\n",
        " 99    1\n",
        "235    1\n",
        "240    1\n",
        "115    1\n",
        " 56    1\n",
        "121    1\n",
        " 63    1\n",
        "----------------\n",
        "Year created"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "2008 1098\n",
        "2009 9786\n",
        "2011 33975\n",
        "2010 22548\n",
        "2012 32593\n",
        "-----------------\n",
        "Overall size (100MB)"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "  0 97413\n",
        "  1  1609\n",
        "  2   441\n",
        "  3   157\n",
        "  4    93\n",
        "  5    66\n",
        "  6    52\n",
        "  7    35\n",
        "  8    19\n",
        "  9    15\n",
        " 10   100\n",
        "--------------\n",
        "[(u'python', 16), (u'c', 6), (u'c++', 5), (u'ruby', 4), (u'objective-c', 2), (u'clojure', 1), (u'haxe', 1), (u'ada', 1), (u'matlab', 1)]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'python', 196), (u'php', 25), (u'java', 25), (u'c', 24), (u'c++', 17), (u'javascript', 16), (u'perl', 16), (u'ruby', 9), (u'ocaml', 6)]\n",
        "[(u'python', 1131), (u'java', 403), (u'c++', 308), (u'php', 266), (u'c#', 259), (u'c', 179), (u'javascript', 131), (u'ruby', 74), (u'shell', 59)]\n",
        "[(u'python', 5857), (u'java', 3280), (u'c++', 2408), (u'c#', 2037), (u'php', 1759), (u'c', 1214), (u'javascript', 1183), (u'html/css', 575), (u'ruby', 543)]\n",
        "[(u'python', 4768), (u'java', 3538), (u'c++', 1969), (u'php', 1955), (u'c#', 1934), (u'javascript', 1382), (u'c', 1255), (u'html/css', 818), (u'ruby', 656)]\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "python, c, c++, ruby, php, java, javascript"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 85
    }
   ],
   "metadata": {}
  }
 ]
}